<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Complete ROC & AUC Derivation</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- Link to the "production-ready" stylesheet -->
  <link rel="stylesheet" href="../css/styles.css">
  <!-- Include MathJax for rendering LaTeX math -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" async></script>
</head>
<body>
  <header>
    <nav>
      <a href="index.html">Home</a>
      <a href="about.html">About</a>
    </nav>
    <div class="container">
      <h1>Complete ROC &amp; AUC Derivation</h1>
      <p><em>By Gabriel Dunin-Borkowski – <span style="white-space:nowrap;">Last Updated: 2025-03-01</span></em></p>
    </div>
  </header>

  <main class="container">

    <!-- ABSTRACT / INTRO TEXT -->
    <section>
      <p><strong>Abstract:</strong> As in the title. Made to clarify my own knowledge, based mostly on the great 
      answer by 
      <a href="https://stats.stackexchange.com/q/277721" target="_blank">Alebu at StackExchange</a>.
      </p>
    </section>

    <hr/>

    <!-- INTRODUCTION SECTION -->
    <section>
      <h2>1. Introduction</h2>
      <p>
        We consider two cumulative distribution functions (CDFs), one for the negative class 
        and one for the positive class:
      </p>
      <p>
$$
F_0(t) := P(S < t\,\vert\, \text{negative}),
$$

$$
F_1(t) := P(S < t \,\vert\, \text{positive}).
$$
      </p>
      <p>
        To make this clearer, think of t as a red line which
        divides two teams (a threshold, if you will). The realizations
        of the random variable S which exceed the threshold t belong
        to the positive team, and realizations below that threshold
        belong to the negative team. This might seem unintuitive, since
        the probability of belonging to the negative class if you belong
        to the negative class is a hundred percent. The way to clarify
        this is to introduce an auxiliary variable v and understand
        that for a given value t, the CDF is actually measuring the
        probability that the score will be below v, up to when v
        catches up with t. So basically, the CDF for the negative class has
        in its domain the values of v up to when v catches up with t.
        Symmetrically, the CDF for the positive class has in its domain
        the values of some w up to when w catches up with t. However,
        as you might notice - this makes no sense, since we would get a CDF
        with constant value equal to zero for the positive class. What's crucial
        here is to understand that the positive and negative classes
        are defined not in terms of theoretical distributions, but
        rather in terms of empirical distributions. So: the CDF of the
        negative class is actually the proportion of the negative class
        correctly classified as negative by the decision rule, and the
        CDF of the positive class is the proportion of the positive class
        incorrectly classified as negative.
      </p>
    </section>

    <hr/>

    <!-- DEFINING FPR AND TPR -->
    <section>
      <h2>2. Defining FPR and TPR</h2>
      <p>
$$
\text{FPR}(t) = x(t) = P(S > t \mid \text{negative}) = 1 - F_0(t),
$$

$$
\text{TPR}(t) = y(t) = P(S > t \mid \text{positive}) = 1 - F_1(t).
$$
      </p>
      <p>
        Hence, the ROC curve is represented by the points 
        \(\bigl(\text{FPR}(t),\ \text{TPR}(t)\bigr)\equiv \bigl(x(t),\ y(t)\bigr)\).
        We might write this as \((\,x(t),\ y(t)\,)\), with \(y(x(t)) = y(t)\).
      </p>
    </section>

    <hr/>

    <!-- ROC AND AUC SECTION -->
    <section>
      <h2>3. ROC and AUC</h2>
      <p>
        <em>(Here we see the integral definition of AUC in terms of \(y\) and \(x\). 
        Essentially, the area under the ROC curve is the integral of the 
        TPR/FPR tradeoff curve.)</em>
      </p>
      <p>
$$
\text{AUC} = \int_{0}^{1} y(x)\,dx,
$$
      </p>
      <p>
        where \(x(t) = 1 - F_0(t)\) and \(y(t) = 1 - F_1(t)\). 
        Taking derivatives w.r.t. \(t\):
      </p>
      <p>
$$
\frac{dx}{dt} = -f_0(t), \quad \frac{dy}{dt} = -f_1(t).
$$

$$
dx = -f_0(t)\,dt,
$$

$$
\int_{0}^{1} y\,dx 
= \int_{0=x(t)}^{1=x(t)} \bigl[\,1 - F_1(t)\bigr]\bigl[-f_0(t)\bigr]\ dt.
$$
      </p>
    </section>

    <hr/>

    <!-- BEHAVIOR OF X(t) -->
    <section>
      <h2>4. Behavior of \(x(t)\) as \(t\) Varies</h2>
      <p>
        <em>(Key insight: \(x(t)\) decreases from 1 to 0 as \(t\) goes from \(-\infty\) to \(+\infty\).)</em>
      </p>
      <p>
        In other words:
      </p>
      <pre>
where x(t) goes from 1 to 0 as t goes from -∞ to +∞.

 x(t) | F₀(t) | t
------+-------+---------
   0  |   1   | +∞
   1  |   0   | -∞

 F₀(t): 1 → 0  (for -∞ → +∞).
      </pre>
    </section>

    <hr/>

    <!-- FULL AUC DERIVATION -->
    <section>
      <h2>5. Full AUC Derivation</h2>
      <p>
        <em>(The integral for AUC in terms of the density \(f_0\) and cumulant \(F_1\).)</em>
      </p>
      <p>
$$
\text{AUC} = \int_{-\infty}^{\infty} \bigl[1 - F_1(t)\bigr]\ f_0(t)\ dt.
$$
      </p>
      <p>
        Where we could get rid of the negative sign due to the change in integration limits. 
        Given \(\mathbf{P}(\Omega)=1\) and \(\mathbf{P}(\emptyset)=0\), an equivalent probabilistic interpretation emerges:
      </p>
      <p>
$$
\text{AUC} = E_{S_0, S_1}\bigl[\mathbf{1}(S_1 > S_0)\bigr]
= \mathbb{E}_{\,S_0 \sim f_0}\bigl[P(S_1 > S_0)\bigr].
$$
      </p>
      <p>
        Interpreting integrals as the probability that a randomly selected positive score \(S_1\) 
        exceeds a randomly selected negative score \(S_0\). 
        Essentially, the AUC is the expected probability that a member of the positive class 
        has a higher score than a member of the negative class.
      </p>
    </section>

    <hr/>

    <!-- CONCLUSION -->
    <section>
      <h2>6. Conclusion</h2>
      <p>
        The derivation starts from \(F_0(t)\) and \(F_1(t)\), 
        defines FPR and TPR, and shows how the area under the ROC curve 
        can also be seen as the probability that a random positive sample 
        out-scores a random negative sample.
      </p>
    </section>

    <hr/>

    <!-- GINI -->
    <section>
      <h2>7. GINI</h2>
      <p>
        Assume \(AUC = \frac{1}{2}\). Then \(2 \cdot AUC - 1 = 0\). 
        Write \(GINI = 2 \cdot AUC - 1.\) 
        Thus, a GINI of <code>0</code> means your predictive power is no better than random; 
        <code>1</code> would be perfect discrimination, and <code>-1</code> would be labels flipped.
      </p>
      <p>
        Intuitively, GINI centers the AUC around zero and scales the halves to ones. 
        That means for two random members of the positive and negative classes, 
        it reflects the average rank difference or how much better your model does 
        compared to chance.
      </p>
    </section>

    <hr/>

    <!-- ALTERNATIVE DERIVATION -->
    <section>
      <h2>8. Alternatively, a Quicker Way</h2>
      <p>
$$
\text{AUC} 
= \int_{-\infty}^{\infty} \text{TPR}(t)\, d\bigl(\text{FPR}(t)\bigr)
= \int_{0}^{1} \text{TPR}\bigl(\text{FPR}^{-1}(p)\bigr)\, dp.
$$
      </p>
      <p>
        Where \(\text{FPR}^{-1}(p)\) is the threshold \(t\) s.t. \(\text{FPR}(t) = p\).  
        Equivalently, using an indicator function approach:
      </p>
      <p>
$$
\text{AUC} 
= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
 \mathbf{1}(x_1 > x_2)\, f_{X_1}(x_1)\, f_{X_2}(x_2)\, dx_1\, dx_2.
$$
      </p>
      <p>
        This can be viewed as an application of the convolution theorem.
      </p>
    </section>

  </main>

  <footer class="container">
    <p>&copy; 2025 Gabriel Dunin-Borkowski. Website under construction.</p>
  </footer>

</body>
</html>
