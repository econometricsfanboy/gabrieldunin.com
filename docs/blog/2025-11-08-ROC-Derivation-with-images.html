---
layout: blog
title: "AUC: the probability view (with derivation and figures)"
mathjax: true
date: 2025-11-08
tags: [roc, auc, probability, visualization, python, machine-learning]
excerpt: "The crux: AUC is the probability that a random positive outranks a random negative. ROC geometry, likelihood-ratio slopes, and invariances fall out of that."
kramdown:
  parse_block_html: true
---

<!-- ABSTRACT -->
<section>
  <p><strong>Picture.</strong> A scoring model assigns each case a real-valued score \(S\). For binary targets (negative \(0\), positive \(1\)), imagine drawing one truly positive and one truly negative at random and asking whether the model ranks the positive higher. The probability of that event is the area under the ROC curve (AUC). This post motivates the statement, derives it cleanly (including ties), and provides Python to generate figures that make the idea tangible.</p>
</section>

<hr/>

<!-- MOTIVATION -->
<section>
  <h2>1. Motivation:</h2>
  <p>Picture a threshold sweeping along a line. Scores live on the line. Let \(F_0\) and \(F_1\) denote the class-conditional CDFs of \(S\) under negatives and positives; \(f_0\), \(f_1\) are their densities where they exist. Sweep a threshold \(t\) from \(+\infty\) to \(-\infty\), predicting “positive” when \(S&gt;t\). The false-positive and true-positive rates are</p>
  \[
  \mathrm{FPR}(t)=\Pr(S&gt;t\mid 0)=1-F_0(t),\qquad
  \mathrm{TPR}(t)=\Pr(S&gt;t\mid 1)=1-F_1(t).
  \]
  <p>Plotting \(\mathrm{TPR}\) against \(\mathrm{FPR}\) as \(t\) varies yields the receiver operating <strong>characteristic</strong> (ROC) curve. Its area is AUC.</p>

  {% capture fig1_code %}
  # Figure 1: Score distributions (negatives vs positives)
  # Save as: assets/img/auc/distributions.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(42)
  neg = rng.normal(0.0, 1.0, 4000)
  pos = rng.normal(1.25, 1.0, 4000)

  bins = np.linspace(min(neg.min(), pos.min()) - 0.5,
                     max(neg.max(), pos.max()) + 0.5, 60)

  plt.figure(figsize=(7, 4.5))
  plt.hist(neg, bins=bins, alpha=0.6, density=True, label="Negative")
  plt.hist(pos, bins=bins, alpha=0.6, density=True, label="Positive")
  plt.xlabel("Score S"); plt.ylabel("Density")
  plt.title("Score distributions")
  plt.legend(); plt.tight_layout()
  plt.savefig("assets/img/auc/distributions.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 1 — code: simulate two overlapping classes and show score distributions"
     lang="python"
     code=fig1_code %}

  {% include figure.html
     src="/assets/img/auc/distributions.png"
     alt="Class-conditional score densities"
     caption="Figure 1 — score distributions (negatives vs positives)" %}
</section>

<hr/>

<!-- THE CRUX -->
<section>
  <h2>2. Thesis:</h2>
  <p><strong>AUC equals the probability that a randomly chosen positive receives a higher score than a randomly chosen negative.</strong> With independent scores \(S_1\sim f_1\) (positive) and \(S_0\sim f_0\):</p>
  \[
  \boxed{\ \mathrm{AUC}=\Pr(S_1&gt;S_0)\ +\ \tfrac{1}{2}\Pr(S_1=S_0)\ }.
  \]
  <p>This is the Wilcoxon–Mann–Whitney (WMW) view of AUC. It explains key invariances immediately: AUC measures <em>ranking</em> (discrimination), not calibration; it is unchanged by any strictly increasing transformation of the scores and by prior-probability shifts.</p>
</section>

<hr/>

<!-- DERIVATION -->
<section>
  <h2>3. Derivation:</h2>
  <p>Start from the geometric definition:</p>
  \[
  \mathrm{AUC}=\int_{0}^{1} y(x)\,dx,\quad x(t)=1-F_0(t),\ \ y(t)=1-F_1(t).
  \]
  <p>Use a Stieltjes integral to cover continuous, discrete, and mixed cases:</p>
  \[
  \mathrm{AUC}=\int_{-\infty}^{+\infty}\big[1-F_1(t)\big]\,dF_0(t)
  =\mathbb{E}_{S_0\sim F_0}\!\big[1-F_1(S_0)\big].
  \]
  <p>Since \(1-F_1(s)=\Pr(S_1&gt;s)\), taking expectation over \(S_0\) yields</p>
  \[
  \mathrm{AUC}=\Pr(S_1&gt;S_0)\ +\ \tfrac{1}{2}\Pr(S_1=S_0),
  \]
  <p>where the \(\tfrac{1}{2}\) term accounts for ties. In the continuous case, \(\Pr(S_1=S_0)=0\) and the familiar \(\Pr(S_1&gt;S_0)\) remains.</p>

  {% capture fig2_code %}
  # Figure 2: Pairwise wins matrix; mean ≈ AUC (with half-credit for ties)
  # Save as: assets/img/auc/pairwise_matrix.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(0)
  neg = np.sort(rng.normal(0.0, 1.0, 250))
  pos = np.sort(rng.normal(1.25, 1.0, 250))

  # Quantize to induce some ties (for visibility)
  neg_q = np.round(neg, 1)
  pos_q = np.round(pos, 1)

  M = (pos_q[:, None] > neg_q[None, :]).astype(float) + 0.5*(pos_q[:, None] == neg_q[None, :])

  plt.figure(figsize=(6, 6))
  plt.imshow(M, aspect="auto", origin="lower", interpolation="nearest")
  plt.colorbar(label="1 if S_pos > S_neg; 0.5 if tie; 0 otherwise")
  plt.xlabel("Negative samples (sorted)"); plt.ylabel("Positive samples (sorted)")
  plt.title(f"Pairwise wins; mean = {M.mean():.3f} ≈ AUC")
  plt.tight_layout(); plt.savefig("assets/img/auc/pairwise_matrix.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 2 — code: visualize AUC as pairwise wins (positives vs negatives)"
     lang="python"
     code=fig2_code %}

  {% include figure.html
     src="/assets/img/auc/pairwise_matrix.png"
     alt="Pairwise wins matrix"
     caption="Figure 2 — AUC as pairwise wins (negatives vs positives)" %}
</section>

<hr/>

<!-- READING THE ROC -->
<section>
  <h2>4. Reading the ROC like a map</h2>
  <p>Differentiate the parametric form (where densities exist): \(dx/dt=-f_0(t)\) and \(dy/dt=-f_1(t)\). The instantaneous slope is the <strong>likelihood ratio</strong> at threshold \(t\):</p>
  \[
  \frac{dy}{dx}=\frac{f_1(t)}{f_0(t)}.
  \]
  <p>Where the curve is steep, small relaxations of the threshold buy many true positives per false positive—this is the Neyman–Pearson lens, in geometry.</p>

  <p><strong>Iso-cost lines and the optimal operating point.</strong> If the positive prevalence is \(\pi_1\) (so \(\pi_0=1-\pi_1\)) and the false-negative/false-positive costs are \(c_{10}\) and \(c_{01}\), then lines of equal expected cost in ROC space have slope</p>
  \[
  m=\frac{\pi_0\,c_{01}}{\pi_1\,c_{10}}.
  \]
  <p>The Bayes-optimal threshold is where the ROC’s tangent has slope \(m\): \(f_1(t)/f_0(t)=m\).</p>

  {% capture fig3_code %}
  # Figure 3: Empirical ROC with a local tangent (slope ≈ likelihood ratio)
  # Save as: assets/img/auc/roc_curve.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(1)
  neg = rng.normal(0.0, 1.0, 4000); pos = rng.normal(1.25, 1.0, 4000)
  scores = np.concatenate([neg, pos])
  ytrue  = np.concatenate([np.zeros_like(neg, dtype=int), np.ones_like(pos, dtype=int)])

  # Minimal ROC from scratch
  order = np.argsort(-scores, kind="mergesort")
  y = ytrue[order]
  P = y.sum(); N = len(y) - P
  tps = np.cumsum(y); fps = np.cumsum(1 - y)
  tpr = np.concatenate(([0.0], tps / P, [1.0]))
  fpr = np.concatenate(([0.0], fps / N, [1.0]))

  # Pick a point near FPR≈0.2 and estimate slope numerically
  idx = np.argmin(np.abs(fpr - 0.2))
  x0, y0 = fpr[idx], tpr[idx]
  i1 = max(1, idx - 3); i2 = min(len(fpr) - 2, idx + 3)
  slope = (tpr[i2] - tpr[i1]) / (fpr[i2] - fpr[i1])

  plt.figure(figsize=(6, 6))
  plt.plot(fpr, tpr, lw=2, label="ROC")
  plt.plot([0, 1], [0, 1], "--", lw=1, label="Chance")
  x = np.array([x0 - 0.15, x0 + 0.15]); yline = y0 + slope * (x - x0)
  plt.plot(x, yline, lw=1, label=f"Tangent slope ≈ {slope:.2f}")
  plt.scatter([x0], [y0], s=30)
  plt.xlim(0, 1); plt.ylim(0, 1)
  plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
  plt.title("ROC and local slope (likelihood ratio)")
  plt.legend(); plt.tight_layout()
  plt.savefig("assets/img/auc/roc_curve.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 3 — code: empirical ROC and a numerical tangent (slope ≈ likelihood ratio)"
     lang="python"
     code=fig3_code %}

  {% include figure.html
     src="/assets/img/auc/roc_curve.png"
     alt="ROC with local tangent; slope approximates likelihood ratio"
     caption="Figure 3 — ROC and local slope (likelihood ratio)" %}

  {% capture fig5_code %}
  # Figure 4: ROC with iso-cost line and cost-weighted optimum (slope m)
  # Save as: assets/img/auc/iso_cost.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(2)
  neg = rng.normal(0.0, 1.0, 6000); pos = rng.normal(1.25, 1.0, 6000)
  scores = np.concatenate([neg, pos])
  ytrue  = np.concatenate([np.zeros_like(neg, dtype=int), np.ones_like(pos, dtype=int)])

  # ROC
  order = np.argsort(-scores, kind="mergesort")
  y = ytrue[order]
  P = y.sum(); N = len(y) - P
  tps = np.cumsum(y); fps = np.cumsum(1 - y)
  tpr = np.concatenate(([0.0], tps / P, [1.0]))
  fpr = np.concatenate(([0.0], fps / N, [1.0]))

  # Iso-cost slope m = (pi0*c01)/(pi1*c10)
  pi1 = 0.2; pi0 = 1 - pi1; c10 = 5.0; c01 = 1.0
  m = (pi0 * c01) / (pi1 * c10)

  # Cost-weighted Youden index: maximize TPR - m*FPR
  idx = np.argmax(tpr - m * fpr)
  x0, y0 = fpr[idx], tpr[idx]

  # Iso-cost line through (x0, y0)
  x = np.array([0.0, 1.0]); y = y0 + m * (x - x0)

  plt.figure(figsize=(6, 6))
  plt.plot(fpr, tpr, lw=2, label="ROC")
  plt.plot([0, 1], [0, 1], "--", lw=1, label="Chance")
  plt.plot(x, y, lw=1, label=f"Iso-cost slope m = {m:.2f}")
  plt.scatter([x0], [y0], s=30, label="Cost-weighted optimum")
  plt.xlim(0, 1); plt.ylim(0, 1)
  plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
  plt.title("ROC with iso-cost line and optimum")
  plt.legend(); plt.tight_layout()
  plt.savefig("assets/img/auc/iso_cost.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 4 — code: iso-cost geometry and cost-weighted operating point"
     lang="python"
     code=fig5_code %}

  {% include figure.html
     src="/assets/img/auc/iso_cost.png"
     alt="ROC overlaid with an iso-cost line and the optimum"
     caption="Figure 4 — Iso-cost slope \(m\) and the implied optimal operating point" %}
</section>

<hr/>

<!-- THREE PICTURES -->
<section>
  <h2>5. Three equivalent pictures of AUC</h2>
  <ol>
    <li><strong>Integral under trade-off:</strong> \(\displaystyle \mathrm{AUC}=\int_0^1 \mathrm{TPR}(x)\,dx\).</li>
    <li><strong>Pairwise probability:</strong> \(\mathrm{AUC}=\Pr(S_1&gt;S_0)+\tfrac{1}{2}\Pr(S_1=S_0)\).</li>
    <li><strong>Difference distribution:</strong> Let \(D=S_1-S_0\); then \(\mathrm{AUC}=\Pr(D&gt;0)+\tfrac{1}{2}\Pr(D=0)\).</li>
  </ol>

  <p>AUC equals Harrell’s <strong>c-index</strong> for binary outcomes; in survival settings, c adapts to censoring.</p>

  {% capture fig4_code %}
  # Figure 5: Distribution of differences D = S_pos - S_neg
  # Save as: assets/img/auc/difference_distribution.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(7)
  neg = rng.normal(0.0, 1.0, 10000)
  pos = rng.normal(1.25, 1.0, 10000)
  d = rng.choice(pos, 20000) - rng.choice(neg, 20000)

  plt.figure(figsize=(7, 4.5))
  plt.hist(d, bins=80, density=True)
  plt.axvline(0.0, linestyle="--")
  plt.xlabel("D = S_pos - S_neg"); plt.ylabel("Density")
  plt.title(f"P(D>0) ≈ {np.mean(d>0):.3f} ≈ AUC")
  plt.tight_layout(); plt.savefig("assets/img/auc/difference_distribution.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 5 — code: difference distribution; area right of 0 equals AUC"
     lang="python"
     code=fig4_code %}

  {% include figure.html
     src="/assets/img/auc/difference_distribution.png"
     alt="Distribution of D = S_pos − S_neg; area right of zero is AUC"
     caption="Figure 5 — \(D=S_{\text{pos}}-S_{\text{neg}}\); \(P(D&gt;0)\) equals AUC" %}
</section>

<hr/>

<!-- PRACTICALITIES -->
<section>
  <h2>6. Practicalities in one place</h2>
  <ul>
    <li><strong>Empirical AUC is pair counting (a WMW \(U\)-statistic).</strong> With \(m\) positives and \(n\) negatives, average \(\mathbf{1}\{s_{1,i}&gt;s_{0,j}\}\) over all \(mn\) cross-pairs; give ties half-credit. DeLong’s method gives a nonparametric variance and tests for comparing ROCs.</li>
    <li><strong>Prevalence-free and rank-only.</strong> AUC conditions on class and depends only on the order of scores; it is invariant to any strictly increasing score transform and to prior shifts.</li>
    <li><strong>Threshold choice is contextual.</strong> The optimal point depends on prevalence and costs: pick the tangent where \(f_1/f_0=(\pi_0 c_{01})/(\pi_1 c_{10})\) (Section 4).</li>
    <li><strong>ROC vs PR.</strong> Under extreme imbalance, precision–recall (PR) curves often communicate utility more directly; PR focuses on the positive class and depends on prevalence. AUC remains well-defined but can overstate performance in rare-positive regimes.</li>
    <li><strong>Gini.</strong> The credit-scoring Gini is \(G=2\mathrm{AUC}-1\).</li>
    <li><strong>Multiclass.</strong> Common practice is one-vs-rest or one-vs-one macro-averaging; interpretability follows the binary case but the meaning of a single scalar “AUC” is aggregation-dependent.</li>
  </ul>

  {% capture minimal_auc_code %}
  # Minimal ROC/AUC from scores and labels
  # Usage: fpr, tpr, auc = roc_auc(y_true, y_score)
  import numpy as np

  def roc_auc(y_true, y_score):
      y_true = np.asarray(y_true).astype(int)
      y_score = np.asarray(y_score).astype(float)
      order = np.argsort(-y_score, kind="mergesort")
      y = y_true[order]
      P = y.sum(); N = len(y) - P
      tps = np.cumsum(y); fps = np.cumsum(1 - y)
      tpr = np.concatenate(([0.0], tps / P, [1.0]))
      fpr = np.concatenate(([0.0], fps / N, [1.0]))
      auc = np.trapz(tpr, fpr)
      return fpr, tpr, auc
  {% endcapture %}

  {% include collapse.html
     title="Minimal ROC/AUC — code"
     lang="python"
     code=minimal_auc_code %}

  {% capture auc_paircount_code %}
  # AUC via explicit pair counting (with half-credit for ties)
  # Works for small/medium data; O(m*n) memory if you materialize the matrix.
  import numpy as np

  def auc_paircount(y_true, y_score):
      y_true = np.asarray(y_true).astype(int)
      y_score = np.asarray(y_score).astype(float)
      pos = y_score[y_true == 1]
      neg = y_score[y_true == 0]
      if len(pos) == 0 or len(neg) == 0:
          raise ValueError("Both classes must be present.")
      # Broadcasting comparison; add 0.5 for ties.
      wins = (pos[:, None] > neg[None, :]).sum()
      ties = (pos[:, None] == neg[None, :]).sum()
      return (wins + 0.5 * ties) / (len(pos) * len(neg))
  {% endcapture %}

  {% include collapse.html
     title="AUC via WMW pair counting — code"
     lang="python"
     code=auc_paircount_code %}

  <p>If you want a library call, see scikit-learn’s <a href="https://www.google.com/search?q=roc_curve+site%3Ascikit-learn.org"><code>roc_curve</code></a> and <a href="https://www.google.com/search?q=roc_auc_score+site%3Ascikit-learn.org"><code>roc_auc_score</code></a>.</p>
</section>

<hr/>

<!-- PROBLEMS -->
<!-- PROBLEMS -->
<section>
  <h2>7. Problem set</h2>
  <ol>
    <li><strong>Stieltjes tie term.</strong> Starting from \(\mathrm{AUC}=\int(1-F_1)\,dF_0\), derive \(\mathrm{AUC}=\Pr(S_1&gt;S_0)+\tfrac{1}{2}\Pr(S_1=S_0)\) without assuming densities. Your derivation must make explicit where the \(\tfrac{1}{2}\) factor comes from.</li>

    <li><strong>Double-integral equivalence.</strong> Show \(\mathrm{AUC}=\iint \mathbf{1}\{s_1&gt;s_0\}f_1(s_1)f_0(s_0)\,ds_1ds_0\) and explain carefully why Fubini/Tonelli applies (what measurability/boundedness suffices here?).</li>

    <li><strong>ROC concavity ⇔ monotone likelihood ratio.</strong> Prove that if \(f_1(t)/f_0(t)\) is increasing in \(t\) (MLR property), then the ROC is concave. Construct a counterexample (non-MLR) where the ROC has a convex segment.</li>

    <li><strong>Bayes operating point from ROC geometry.</strong> For prevalence \(\pi\) and costs \(C_{10}\) (FP), \(C_{01}\) (FN), show that the Bayes-optimal threshold lies where the ROC’s tangent slope equals \(\frac{C_{10}(1-\pi)}{C_{01}\pi}\). Then, for \(S\mid Y=i\sim \mathcal{N}(\mu_i,\sigma^2)\), derive the closed-form threshold \(t^\star\) and verify that the slope there equals the cost ratio.</li>

    <li><strong>Binormal AUC in one line.</strong> If \(S_1\sim\mathcal{N}(\mu_1,\sigma_1^2)\), \(S_0\sim\mathcal{N}(\mu_0,\sigma_0^2)\) independent, show \(\mathrm{AUC}=\Phi\!\left(\dfrac{\mu_1-\mu_0}{\sqrt{\sigma_1^2+\sigma_0^2}}\right)\). (Hint: consider \(D=S_1-S_0\).)</li>

    <li><strong>Strictly increasing vs. non-strictly increasing transforms.</strong> Prove that any strictly increasing \(g\) leaves AUC invariant. Give an explicit non-strictly increasing \(g\) (e.g., clipping/quantization) that reduces AUC via ties, and quantify the reduction on a simple two-point score distribution.</li>

    <li><strong>Quantization penalty.</strong> Suppose continuous scores are quantized to \(b\) bits (uniform bins). Provide an upper bound on the AUC drop due to ties as a function of \(b\) and the overlap of \(F_0, F_1\). (A tight big-O answer with clear assumptions earns full credit.)</li>

    <li><strong>Pairwise/U-statistic variance.</strong> Treat empirical AUC as a U-statistic with kernel \(h(s_1,s_0)=\mathbf{1}\{s_1&gt;s_0\}+\tfrac{1}{2}\mathbf{1}\{s_1=s_0\}\). Show unbiasedness and derive its asymptotic variance in terms of \(\operatorname{Var}(\mathbb{E}[h\mid S_1])\) and \(\operatorname{Var}(\mathbb{E}[h\mid S_0])\). State regularity conditions.</li>

    <li><strong>Symmetric label noise.</strong> Assume class-independent flip rate \(\eta\in[0,\tfrac12)\). Let \(\mathrm{AUC}_\text{true}\) be the population AUC and \(\mathrm{AUC}_\text{obs}\) the AUC measured against noisy labels. Show
      \[
      \mathrm{AUC}_\text{obs}=(1-2\eta)\,\mathrm{AUC}_\text{true}+\eta
      \]
      (continuous-score case). Interpret the linear shrinkage toward \(0.5\).</li>

    <li><strong>Partial AUC as a weighted probability.</strong> For normalized pAUC over \(\mathrm{FPR}\in[0,\alpha]\),
      \[
      \mathrm{pAUC}_\alpha=\frac{1}{\alpha}\int_{0}^{\alpha}\mathrm{TPR}(x)\,dx,
      \]
      derive a pairwise representation that samples negatives only from the top \(\alpha\) FPR slice (i.e., highest-scoring \(\alpha\) fraction of negatives) with appropriate normalization. State precisely how ties at the slice boundary are handled.</li>

    <li><strong>Youden’s \(J\) vs cost-optimal.</strong> Show on a concrete family (e.g., equal-variance Gaussians) that the threshold maximizing \(J=\mathrm{TPR}-\mathrm{FPR}\) generally differs from the Bayes-optimal threshold unless \(C_{10}(1-\pi)=C_{01}\pi\). Quantify the regret in expected cost when using \(J\) instead of Bayes-optimal.</li>

    <li><strong>Same AUC, different calibration.</strong> Construct two scorers with identical AUC but different calibration curves. Verify equal AUC via the rank-based formula, and contrast Brier score / ECE. Explain why AUC cannot adjudicate between them for decision-making at fixed cost ratios.</li>

    <li><strong>Mixtures and Simpson-type reversals.</strong> Give subgroup-specific distributions \((F_0^{(g)},F_1^{(g)})\) with \(\mathrm{AUC}^{(1)}&gt;\mathrm{AUC}^{(2)}\) but overall \(\mathrm{AUC}\) reverses after mixing subgroups due to different subgroup weights. Prove the reversal algebraically.</li>

    <li><strong>ROC slope as likelihood ratio—finite-difference test.</strong> On simulated data, estimate the ROC slope at threshold \(t\) by symmetric finite differences. Independently estimate \(f_1(t)/f_0(t)\) by kernel density ratios. Show convergence of the two estimates as sample size grows and discuss boundary effects.</li>

    <li><strong>Ranking surrogates and AUC.</strong> Consider pairwise logistic loss \(\ell(s_1,s_0)=\log\!\bigl(1+\exp(-(s_1-s_0))\bigr)\). Show that minimizing \(\mathbb{E}[\ell]\) is classification-calibrated for AUC (i.e., minimizes misranking risk in the limit). Provide the key inequality linking surrogate excess risk to AUC regret.</li>

    <li><strong>Class-conditional monotone transforms.</strong> Show by counterexample that applying different strictly increasing transforms \(g_1,g_0\) to positives and negatives can change AUC, even though each \(g_i\) is monotone. Explain the mechanism in terms of LR reweighting.</li>

    <li><strong>AUC under covariate shift.</strong> If \(p(x\mid Y)\) changes but the conditional score distributions \(F_i\) induced by the model remain unchanged, argue why ROC/AUC stays invariant while PR/AUPRC can change. Give a small constructed example.</li>
  </ol>

  <!-- SOLUTIONS DROPDOWN -->
  <details>
    <summary><strong>One‑line solutions (click to expand)</strong></summary>
    <ol>
      <li><strong>Stieltjes tie term.</strong> Ties get half‑credit because when sweeping a threshold past an atom, the Riemann–Stieltjes integral splits the jump evenly; equivalently, an infinitesimal random perturbation breaks any tie \(50\text{–}50\).</li>

      <li><strong>Double‑integral equivalence.</strong> Since \(0\le \mathbf{1}\{s_1&gt;s_0\}\le 1\) and \(f_0,f_1\) are integrable densities, Tonelli applies; thus \(\iint \mathbf{1}\{s_1&gt;s_0\}f_1(s_1)f_0(s_0)\,ds_1ds_0=\Pr(S_1&gt;S_0)\).</li>

      <li><strong>ROC concavity ⇔ MLR.</strong> Parameterize ROC by threshold \(t\). Then \(\dfrac{d\,\mathrm{TPR}(t)}{d\,\mathrm{FPR}(t)}=\dfrac{f_1(t)}{f_0(t)}\). If \(f_1/f_0\) increases in \(t\), the slope decreases as FPR increases, hence the ROC is concave; a non‑MLR pair (e.g., bimodal \(f_1\) crossing \(f_0\) twice) yields a convex bump.</li>

      <li><strong>Bayes operating point.</strong> Choose \(t^\star\) where \(\dfrac{f_1(t^\star)}{f_0(t^\star)}=\dfrac{C_{10}(1-\pi)}{C_{01}\pi}\). For \(S_i\sim\mathcal{N}(\mu_i,\sigma^2)\),
        \[
        t^\star=\frac{\mu_0+\mu_1}{2}+\frac{\sigma^2}{\mu_1-\mu_0}\ln\!\frac{(1-\pi)C_{10}}{\pi C_{01}},
        \]
        and the ROC slope there equals the cost ratio.</li>

      <li><strong>Binormal AUC.</strong> With independence \(D=S_1-S_0\sim\mathcal{N}(\mu_1-\mu_0,\sigma_1^2+\sigma_0^2)\), so \(\mathrm{AUC}=\Pr(D&gt;0)=\Phi\!\big((\mu_1-\mu_0)/\sqrt{\sigma_1^2+\sigma_0^2}\big)\).</li>

      <li><strong>Strictly vs. non‑strict transforms.</strong> Any strictly increasing \(g\) preserves all pairwise orders (AUC invariant). A non‑strict map (rounding/clipping) converts a fraction \(p\) of wins into ties, reducing AUC by \(p/2\).</li>

      <li><strong>Quantization penalty.</strong> With uniform bin width \(w\approx\text{range}/2^b\),
        \[
        \Delta\mathrm{AUC}\le \tfrac12\sum_k \Pr(S_1\in k)\Pr(S_0\in k)\approx \frac{w}{2}\int f_1(x)f_0(x)\,dx=O(2^{-b}),
        \]
        under mild smoothness.</li>

      <li><strong>U‑statistic variance.</strong> Empirical AUC (with tie half‑credit) is an unbiased \((1,1)\) U‑statistic with asymptotic variance \(\operatorname{Var}(\psi_1(S_1))/m+\operatorname{Var}(\psi_0(S_0))/n\), where \(\psi_1(s)=\mathbb{E}[h(s,S_0)]\), \(\psi_0(s)=\mathbb{E}[h(S_1,s)]\).</li>

      <li><strong>Symmetric label noise.</strong> Each pair is correct with prob.\((1-2\eta)\) and same‑label with prob.\(\eta\); hence \(\mathrm{AUC}_\text{obs}=(1-2\eta)\,\mathrm{AUC}_\text{true}+\eta\), i.e., linear shrinkage toward \(0.5\).</li>

      <li><strong>Partial AUC.</strong> Let \(\mathcal{N}_\alpha\) be negatives in the top \(\alpha\) FPR slice. Then
        \[
        \mathrm{pAUC}_\alpha=\Pr(S_1&gt;S_0\mid S_0\in\mathcal{N}_\alpha)+\tfrac12\Pr(S_1=S_0\mid S_0\in\mathcal{N}_\alpha),
        \]
        with boundary ties fractionally included so the FPR mass is exactly \(\alpha\).</li>

      <li><strong>Youden’s \(J\) vs. Bayes.</strong> \(J\) selects slope \(1\), whereas Bayes uses \(m=\frac{C_{10}(1-\pi)}{C_{01}\pi}\). Regret in expected cost:
        \[
        C_{01}\pi\,(\mathrm{TPR}^\star-\mathrm{TPR}_J)+C_{10}(1-\pi)\,(\mathrm{FPR}_J-\mathrm{FPR}^\star).
        \]</li>

      <li><strong>Same AUC, different calibration.</strong> Identical rankings \(\Rightarrow\) same AUC, yet Brier/ECE can differ markedly; AUC cannot decide between calibrated probabilities for fixed cost ratios.</li>

      <li><strong>Mixtures & Simpson reversals.</strong> Overall \(\mathrm{AUC}=\sum_{g,g'} w_g w_{g'}\,\Pr(S_1^{(g)}&gt;S_0^{(g')})+\tfrac12\Pr(=)\). Changing subgroup weights \(w_g\) can reverse model ordering even if each subgroup prefers the opposite.</li>

      <li><strong>ROC slope = LR (finite differences).</strong> Symmetric finite differences \(\Delta\mathrm{TPR}/\Delta\mathrm{FPR}\) around \(t\) converge to \(f_1(t)/f_0(t)\) as \(n\to\infty\); bias/variance inflate near boundaries where densities are small.</li>

      <li><strong>Ranking surrogate calibration.</strong> Pairwise logistic loss is classification‑calibrated for AUC: there exists \(\psi\) with \(\psi(\text{AUC regret})\le \text{logistic regret}\); minimizing the surrogate drives misranking risk to zero.</li>

      <li><strong>Class‑conditional monotone maps.</strong> Applying different strictly increasing \(g_1,g_0\) changes Jacobians and hence the likelihood ratio \(f_1/f_0\), altering pairwise orders; AUC can change.</li>

      <li><strong>Covariate shift.</strong> If score marginals \(F_0,F_1\) are unchanged, ROC/AUC (conditioned on class) remain invariant, while PR/AUPRC shift with prevalence via \(\mathrm{Prec}=\frac{\pi\,\mathrm{TPR}}{\pi\,\mathrm{TPR}+(1-\pi)\,\mathrm{FPR}}\).</li>
    </ol>
  </details>
</section>
