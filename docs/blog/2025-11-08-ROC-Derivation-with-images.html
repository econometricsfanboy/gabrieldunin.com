---
layout: blog
title: "AUC: the probability view (with derivation and figures)"
mathjax: true
date: 2025-11-08
tags: [roc, auc, probability, visualization, python, machine-learning]
excerpt: "The crux: AUC is the probability that a random positive outranks a random negative. ROC geometry, likelihood-ratio slopes, and invariances fall out of that."
kramdown:
  parse_block_html: true
---

<!-- ABSTRACT -->
<section>
  <p><strong>Picture.</strong> A scoring model assigns each case a real-valued score \(S\). For binary targets (negative \(0\), positive \(1\)), imagine drawing one truly positive and one truly negative at random and asking whether the model ranks the positive higher. The probability of that event is the area under the ROC curve (AUC). This post motivates the statement, derives it cleanly (including ties), and provides Python to generate figures that make the idea tangible.</p>
</section>

<hr/>

<!-- MOTIVATION -->
<section>
  <h2>1. Motivation:</h2>
  <p>Picture a threshold sweeping along a line. Scores live on the line. Let \(F_0\) and \(F_1\) denote the class-conditional CDFs of \(S\) under negatives and positives; \(f_0\), \(f_1\) are their densities where they exist. Sweep a threshold \(t\) from \(+\infty\) to \(-\infty\), predicting “positive” when \(S&gt;t\). The false-positive and true-positive rates are</p>
  \[
  \mathrm{FPR}(t)=\Pr(S&gt;t\mid 0)=1-F_0(t),\qquad
  \mathrm{TPR}(t)=\Pr(S&gt;t\mid 1)=1-F_1(t).
  \]
  <p>Plotting \(\mathrm{TPR}\) against \(\mathrm{FPR}\) as \(t\) varies yields the receiver operating <strong>characteristic</strong> (ROC) curve. Its area is AUC.</p>

  {% capture fig1_code %}
  # Figure 1: Score distributions (negatives vs positives)
  # Save as: assets/img/auc/distributions.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(42)
  neg = rng.normal(0.0, 1.0, 4000)
  pos = rng.normal(1.25, 1.0, 4000)

  bins = np.linspace(min(neg.min(), pos.min()) - 0.5,
                     max(neg.max(), pos.max()) + 0.5, 60)

  plt.figure(figsize=(7, 4.5))
  plt.hist(neg, bins=bins, alpha=0.6, density=True, label="Negative")
  plt.hist(pos, bins=bins, alpha=0.6, density=True, label="Positive")
  plt.xlabel("Score S"); plt.ylabel("Density")
  plt.title("Score distributions")
  plt.legend(); plt.tight_layout()
  plt.savefig("assets/img/auc/distributions.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 1 — code: simulate two overlapping classes and show score distributions"
     lang="python"
     code=fig1_code %}

  {% include figure.html
     src="/assets/img/auc/distributions.png"
     alt="Class-conditional score densities"
     caption="Figure 1 — score distributions (negatives vs positives)" %}
</section>

<hr/>

<!-- THE CRUX -->
<section>
  <h2>2. Thesis:</h2>
  <p><strong>AUC equals the probability that a randomly chosen positive receives a higher score than a randomly chosen negative.</strong> With independent scores \(S_1\sim f_1\) (positive) and \(S_0\sim f_0\):</p>
  \[
  \boxed{\ \mathrm{AUC}=\Pr(S_1&gt;S_0)\ +\ \tfrac{1}{2}\Pr(S_1=S_0)\ }.
  \]
  <p>This is the Wilcoxon–Mann–Whitney (WMW) view of AUC. It explains key invariances immediately: AUC measures <em>ranking</em> (discrimination), not calibration; it is unchanged by any strictly increasing transformation of the scores and by prior-probability shifts.</p>
</section>

<hr/>

<!-- DERIVATION -->
<section>
  <h2>3. Derivation:</h2>
  <p>Start from the geometric definition:</p>
  \[
  \mathrm{AUC}=\int_{0}^{1} y(x)\,dx,\quad x(t)=1-F_0(t),\ \ y(t)=1-F_1(t).
  \]
  <p>Use a Stieltjes integral to cover continuous, discrete, and mixed cases:</p>
  \[
  \mathrm{AUC}=\int_{-\infty}^{+\infty}\big[1-F_1(t)\big]\,dF_0(t)
  =\mathbb{E}_{S_0\sim F_0}\!\big[1-F_1(S_0)\big].
  \]
  <p>Since \(1-F_1(s)=\Pr(S_1&gt;s)\), taking expectation over \(S_0\) yields</p>
  \[
  \mathrm{AUC}=\Pr(S_1&gt;S_0)\ +\ \tfrac{1}{2}\Pr(S_1=S_0),
  \]
  <p>where the \(\tfrac{1}{2}\) term accounts for ties. In the continuous case, \(\Pr(S_1=S_0)=0\) and the familiar \(\Pr(S_1&gt;S_0)\) remains.</p>

  {% capture fig2_code %}
  # Figure 2: Pairwise wins matrix; mean ≈ AUC (with half-credit for ties)
  # Save as: assets/img/auc/pairwise_matrix.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(0)
  neg = np.sort(rng.normal(0.0, 1.0, 250))
  pos = np.sort(rng.normal(1.25, 1.0, 250))

  # Quantize to induce some ties (for visibility)
  neg_q = np.round(neg, 1)
  pos_q = np.round(pos, 1)

  M = (pos_q[:, None] > neg_q[None, :]).astype(float) + 0.5*(pos_q[:, None] == neg_q[None, :])

  plt.figure(figsize=(6, 6))
  plt.imshow(M, aspect="auto", origin="lower", interpolation="nearest")
  plt.colorbar(label="1 if S_pos > S_neg; 0.5 if tie; 0 otherwise")
  plt.xlabel("Negative samples (sorted)"); plt.ylabel("Positive samples (sorted)")
  plt.title(f"Pairwise wins; mean = {M.mean():.3f} ≈ AUC")
  plt.tight_layout(); plt.savefig("assets/img/auc/pairwise_matrix.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 2 — code: visualize AUC as pairwise wins (positives vs negatives)"
     lang="python"
     code=fig2_code %}

  {% include figure.html
     src="/assets/img/auc/pairwise_matrix.png"
     alt="Pairwise wins matrix"
     caption="Figure 2 — AUC as pairwise wins (negatives vs positives)" %}
</section>

<hr/>

<!-- READING THE ROC -->
<section>
  <h2>4. Reading the ROC like a map</h2>
  <p>Differentiate the parametric form (where densities exist): \(dx/dt=-f_0(t)\) and \(dy/dt=-f_1(t)\). The instantaneous slope is the <strong>likelihood ratio</strong> at threshold \(t\):</p>
  \[
  \frac{dy}{dx}=\frac{f_1(t)}{f_0(t)}.
  \]
  <p>Where the curve is steep, small relaxations of the threshold buy many true positives per false positive—this is the Neyman–Pearson lens, in geometry.</p>

  <p><strong>Iso-cost lines and the optimal operating point.</strong> If the positive prevalence is \(\pi_1\) (so \(\pi_0=1-\pi_1\)) and the false-negative/false-positive costs are \(c_{10}\) and \(c_{01}\), then lines of equal expected cost in ROC space have slope</p>
  \[
  m=\frac{\pi_0\,c_{01}}{\pi_1\,c_{10}}.
  \]
  <p>The Bayes-optimal threshold is where the ROC’s tangent has slope \(m\): \(f_1(t)/f_0(t)=m\).</p>

  {% capture fig3_code %}
  # Figure 3: Empirical ROC with a local tangent (slope ≈ likelihood ratio)
  # Save as: assets/img/auc/roc_curve.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(1)
  neg = rng.normal(0.0, 1.0, 4000); pos = rng.normal(1.25, 1.0, 4000)
  scores = np.concatenate([neg, pos])
  ytrue  = np.concatenate([np.zeros_like(neg, dtype=int), np.ones_like(pos, dtype=int)])

  # Minimal ROC from scratch
  order = np.argsort(-scores, kind="mergesort")
  y = ytrue[order]
  P = y.sum(); N = len(y) - P
  tps = np.cumsum(y); fps = np.cumsum(1 - y)
  tpr = np.concatenate(([0.0], tps / P, [1.0]))
  fpr = np.concatenate(([0.0], fps / N, [1.0]))

  # Pick a point near FPR≈0.2 and estimate slope numerically
  idx = np.argmin(np.abs(fpr - 0.2))
  x0, y0 = fpr[idx], tpr[idx]
  i1 = max(1, idx - 3); i2 = min(len(fpr) - 2, idx + 3)
  slope = (tpr[i2] - tpr[i1]) / (fpr[i2] - fpr[i1])

  plt.figure(figsize=(6, 6))
  plt.plot(fpr, tpr, lw=2, label="ROC")
  plt.plot([0, 1], [0, 1], "--", lw=1, label="Chance")
  x = np.array([x0 - 0.15, x0 + 0.15]); yline = y0 + slope * (x - x0)
  plt.plot(x, yline, lw=1, label=f"Tangent slope ≈ {slope:.2f}")
  plt.scatter([x0], [y0], s=30)
  plt.xlim(0, 1); plt.ylim(0, 1)
  plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
  plt.title("ROC and local slope (likelihood ratio)")
  plt.legend(); plt.tight_layout()
  plt.savefig("assets/img/auc/roc_curve.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 3 — code: empirical ROC and a numerical tangent (slope ≈ likelihood ratio)"
     lang="python"
     code=fig3_code %}

  {% include figure.html
     src="/assets/img/auc/roc_curve.png"
     alt="ROC with local tangent; slope approximates likelihood ratio"
     caption="Figure 3 — ROC and local slope (likelihood ratio)" %}

  {% capture fig5_code %}
  # Figure 4: ROC with iso-cost line and cost-weighted optimum (slope m)
  # Save as: assets/img/auc/iso_cost.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(2)
  neg = rng.normal(0.0, 1.0, 6000); pos = rng.normal(1.25, 1.0, 6000)
  scores = np.concatenate([neg, pos])
  ytrue  = np.concatenate([np.zeros_like(neg, dtype=int), np.ones_like(pos, dtype=int)])

  # ROC
  order = np.argsort(-scores, kind="mergesort")
  y = ytrue[order]
  P = y.sum(); N = len(y) - P
  tps = np.cumsum(y); fps = np.cumsum(1 - y)
  tpr = np.concatenate(([0.0], tps / P, [1.0]))
  fpr = np.concatenate(([0.0], fps / N, [1.0]))

  # Iso-cost slope m = (pi0*c01)/(pi1*c10)
  pi1 = 0.2; pi0 = 1 - pi1; c10 = 5.0; c01 = 1.0
  m = (pi0 * c01) / (pi1 * c10)

  # Cost-weighted Youden index: maximize TPR - m*FPR
  idx = np.argmax(tpr - m * fpr)
  x0, y0 = fpr[idx], tpr[idx]

  # Iso-cost line through (x0, y0)
  x = np.array([0.0, 1.0]); y = y0 + m * (x - x0)

  plt.figure(figsize=(6, 6))
  plt.plot(fpr, tpr, lw=2, label="ROC")
  plt.plot([0, 1], [0, 1], "--", lw=1, label="Chance")
  plt.plot(x, y, lw=1, label=f"Iso-cost slope m = {m:.2f}")
  plt.scatter([x0], [y0], s=30, label="Cost-weighted optimum")
  plt.xlim(0, 1); plt.ylim(0, 1)
  plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
  plt.title("ROC with iso-cost line and optimum")
  plt.legend(); plt.tight_layout()
  plt.savefig("assets/img/auc/iso_cost.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 4 — code: iso-cost geometry and cost-weighted operating point"
     lang="python"
     code=fig5_code %}

  {% include figure.html
     src="/assets/img/auc/iso_cost.png"
     alt="ROC overlaid with an iso-cost line and the optimum"
     caption="Figure 4 — Iso-cost slope \(m\) and the implied optimal operating point" %}
</section>

<hr/>

<!-- THREE PICTURES -->
<section>
  <h2>5. Three equivalent pictures of AUC</h2>
  <ol>
    <li><strong>Integral under trade-off:</strong> \(\displaystyle \mathrm{AUC}=\int_0^1 \mathrm{TPR}(x)\,dx\).</li>
    <li><strong>Pairwise probability:</strong> \(\mathrm{AUC}=\Pr(S_1&gt;S_0)+\tfrac{1}{2}\Pr(S_1=S_0)\).</li>
    <li><strong>Difference distribution:</strong> Let \(D=S_1-S_0\); then \(\mathrm{AUC}=\Pr(D&gt;0)+\tfrac{1}{2}\Pr(D=0)\).</li>
  </ol>

  <p>AUC equals Harrell’s <strong>c-index</strong> for binary outcomes; in survival settings, c adapts to censoring.</p>

  {% capture fig4_code %}
  # Figure 5: Distribution of differences D = S_pos - S_neg
  # Save as: assets/img/auc/difference_distribution.png
  import os, numpy as np, matplotlib.pyplot as plt
  os.makedirs("assets/img/auc", exist_ok=True)

  rng = np.random.default_rng(7)
  neg = rng.normal(0.0, 1.0, 10000)
  pos = rng.normal(1.25, 1.0, 10000)
  d = rng.choice(pos, 20000) - rng.choice(neg, 20000)

  plt.figure(figsize=(7, 4.5))
  plt.hist(d, bins=80, density=True)
  plt.axvline(0.0, linestyle="--")
  plt.xlabel("D = S_pos - S_neg"); plt.ylabel("Density")
  plt.title(f"P(D>0) ≈ {np.mean(d>0):.3f} ≈ AUC")
  plt.tight_layout(); plt.savefig("assets/img/auc/difference_distribution.png", dpi=150); plt.close()
  {% endcapture %}

  {% include collapse.html
     title="Figure 5 — code: difference distribution; area right of 0 equals AUC"
     lang="python"
     code=fig4_code %}

  {% include figure.html
     src="/assets/img/auc/difference_distribution.png"
     alt="Distribution of D = S_pos − S_neg; area right of zero is AUC"
     caption="Figure 5 — \(D=S_{\text{pos}}-S_{\text{neg}}\); \(P(D&gt;0)\) equals AUC" %}
</section>

<hr/>

<!-- PRACTICALITIES -->
<section>
  <h2>6. Practicalities in one place</h2>
  <ul>
    <li><strong>Empirical AUC is pair counting (a WMW \(U\)-statistic).</strong> With \(m\) positives and \(n\) negatives, average \(\mathbf{1}\{s_{1,i}&gt;s_{0,j}\}\) over all \(mn\) cross-pairs; give ties half-credit. DeLong’s method gives a nonparametric variance and tests for comparing ROCs.</li>
    <li><strong>Prevalence-free and rank-only.</strong> AUC conditions on class and depends only on the order of scores; it is invariant to any strictly increasing score transform and to prior shifts.</li>
    <li><strong>Threshold choice is contextual.</strong> The optimal point depends on prevalence and costs: pick the tangent where \(f_1/f_0=(\pi_0 c_{01})/(\pi_1 c_{10})\) (Section 4).</li>
    <li><strong>ROC vs PR.</strong> Under extreme imbalance, precision–recall (PR) curves often communicate utility more directly; PR focuses on the positive class and depends on prevalence. AUC remains well-defined but can overstate performance in rare-positive regimes.</li>
    <li><strong>Gini.</strong> The credit-scoring Gini is \(G=2\mathrm{AUC}-1\).</li>
    <li><strong>Multiclass.</strong> Common practice is one-vs-rest or one-vs-one macro-averaging; interpretability follows the binary case but the meaning of a single scalar “AUC” is aggregation-dependent.</li>
  </ul>

  {% capture minimal_auc_code %}
  # Minimal ROC/AUC from scores and labels
  # Usage: fpr, tpr, auc = roc_auc(y_true, y_score)
  import numpy as np

  def roc_auc(y_true, y_score):
      y_true = np.asarray(y_true).astype(int)
      y_score = np.asarray(y_score).astype(float)
      order = np.argsort(-y_score, kind="mergesort")
      y = y_true[order]
      P = y.sum(); N = len(y) - P
      tps = np.cumsum(y); fps = np.cumsum(1 - y)
      tpr = np.concatenate(([0.0], tps / P, [1.0]))
      fpr = np.concatenate(([0.0], fps / N, [1.0]))
      auc = np.trapz(tpr, fpr)
      return fpr, tpr, auc
  {% endcapture %}

  {% include collapse.html
     title="Minimal ROC/AUC — code"
     lang="python"
     code=minimal_auc_code %}

  {% capture auc_paircount_code %}
  # AUC via explicit pair counting (with half-credit for ties)
  # Works for small/medium data; O(m*n) memory if you materialize the matrix.
  import numpy as np

  def auc_paircount(y_true, y_score):
      y_true = np.asarray(y_true).astype(int)
      y_score = np.asarray(y_score).astype(float)
      pos = y_score[y_true == 1]
      neg = y_score[y_true == 0]
      if len(pos) == 0 or len(neg) == 0:
          raise ValueError("Both classes must be present.")
      # Broadcasting comparison; add 0.5 for ties.
      wins = (pos[:, None] > neg[None, :]).sum()
      ties = (pos[:, None] == neg[None, :]).sum()
      return (wins + 0.5 * ties) / (len(pos) * len(neg))
  {% endcapture %}

  {% include collapse.html
     title="AUC via WMW pair counting — code"
     lang="python"
     code=auc_paircount_code %}

  <p>If you want a library call, see scikit-learn’s <a href="https://www.google.com/search?q=roc_curve+site%3Ascikit-learn.org"><code>roc_curve</code></a> and <a href="https://www.google.com/search?q=roc_auc_score+site%3Ascikit-learn.org"><code>roc_auc_score</code></a>.</p>
</section>

<hr/>

<!-- PROBLEMS -->

<section>
  <h2>Four orthogonal problems</h2>
  <ol>
    <li><strong>Foundations (measure/Stieltjes): the tie term.</strong> Starting from
      \[
        \mathrm{AUC}=\int_{\mathbb{R}} (1-F_1)\,dF_0
      \]
      (Riemann–Stieltjes with the trapezoidal convention), prove
      \[
        \mathrm{AUC}=\Pr(S_1>S_0)+\tfrac{1}{2}\Pr(S_1=S_0)
      \]
      without assuming densities, and make explicit where the \(\tfrac12\) comes from.</li>

<li><strong>Geometry/decision theory: Bayes operating point from ROC slope.</strong>
  With prevalence \(\pi\in(0,1)\) and costs \(C_{10}\) (FP), \(C_{01}\) (FN), show that the Bayes‑optimal threshold \(t^\star\) occurs where the ROC’s tangent slope equals
  \[
    m^\star=\frac{C_{10}(1-\pi)}{C_{01}\pi}.
  \]
  Then, for the equal‑variance binormal model \(S\mid Y=i\sim\mathcal{N}(\mu_i,\sigma^2)\), derive
  \[
    t^\star=\frac{\mu_0+\mu_1}{2}+\frac{\sigma^2}{\mu_1-\mu_0}\,\ln\!\frac{(1-\pi)C_{10}}{\pi C_{01}},
  \]
  and verify that the ROC slope at \(t^\star\) equals \(m^\star\).</li>

<li><strong>Asymptotics/U‑statistics: variance of empirical AUC.</strong>
  Let \(\widehat{\mathrm{AUC}}\) be the empirical AUC computed on \(m\) positives and \(n\) negatives using the kernel
  \[
  h(s_1,s_0)=\mathbf{1}\{s_1>s_0\}+\tfrac12\mathbf{1}\{s_1=s_0\}.
  \]
  Show unbiasedness and derive its asymptotic variance in terms of
  \(\operatorname{Var}(\mathbb{E}[h\mid S_1])\) and \(\operatorname{Var}(\mathbb{E}[h\mid S_0])\).
  State clear regularity conditions.</li>

<li><strong>Distribution shift: AUC invariance under covariate/prevalence shift; PR sensitivity.</strong>
  Suppose the model’s conditional score laws \(F_0,F_1\) are unchanged across domains but the class prior changes from \(\pi\) to \(\pi'\) (\(\pi\neq\pi'\)). Prove that the ROC/AUC is invariant while precision–recall (and AUPRC) can change. Give a concrete numeric example at a fixed threshold to illustrate the magnitude of the precision change.</li>


  </ol>

  <!-- SOLUTIONS DROPDOWN -->

  <details>
    <summary><strong>Solutions (click to expand)</strong></summary>
    <ol>
      <li><strong>Foundations (measure/Stieltjes): the tie term.</strong>
        <p><em>Goal.</em> From \(\displaystyle \mathrm{AUC}=\int (1-F_1)\,dF_0\) show
        \(\mathrm{AUC}=\Pr(S_1>S_0)+\tfrac12\Pr(S_1=S_0)\).</p>

    <p><em>Step 1 (integration by parts with the trapezoidal convention).</em>
    For càdlàg \(F_0,F_1\), Young’s (symmetric) Stieltjes integration by parts gives
    \[
      \int (1-F_1)\,dF_0-\int F_0\,dF_1
      \;=\;
      \underbrace{[(1\!-\!F_1)F_0]_{-\infty}^{+\infty}}_{=\,0}
      \;-\;\tfrac12 \sum_{t\in\mathbb{R}} \Delta F_1(t)\,\Delta F_0(t),
    \]
    where \(\Delta F_i(t)=F_i(t)-F_i(t^-)=\Pr(S_i=t)\). Hence
    \[
      \int (1-F_1)\,dF_0 \;=\; \int F_0\,dF_1 \;-\;\tfrac12 \sum_t \Delta F_1(t)\,\Delta F_0(t).
    \]</p>

    <p><em>Step 2 (identify the two terms probabilistically).</em>
    By the Lebesgue–Stieltjes identity,
    \[
      \int F_0\,dF_1=\mathbb{E}[F_0(S_1)]
      =\mathbb{E}\big[\Pr(S_0\le S_1\mid S_1)\big]
      =\Pr(S_0\le S_1)=\Pr(S_1\ge S_0).
    \]
    Independence of \(S_1\) and \(S_0\) gives
    \(\sum_t \Delta F_1(t)\Delta F_0(t)=\Pr(S_1=S_0)\).
    Therefore
    \[
      \int (1-F_1)\,dF_0
      \;=\; \Pr(S_1\ge S_0)\;-\;\tfrac12\,\Pr(S_1=S_0)
      \;=\; \Pr(S_1>S_0)+\tfrac12\,\Pr(S_1=S_0).
    \]
    <em>The factor \(\tfrac12\) is exactly the cross‑jump term coming from the trapezoidal (midpoint) Stieltjes convention; geometrically it is the area contributed by vertical steps of the ROC when both classes have an atom at the same score.</em></p>
  </li>

  <li><strong>Geometry/decision theory: Bayes operating point from ROC slope.</strong>
    <p><em>Risk as a function of threshold.</em>
    Predict \(1\) when \(S>t\). Expected cost
    \[
      R(t)=C_{01}\,\pi\,F_1(t)+C_{10}\,(1-\pi)\,[1-F_0(t)]
      =C_{01}\pi\,\big(1-\mathrm{TPR}(t)\big)
        +C_{10}(1-\pi)\,\mathrm{FPR}(t).
    \]</p>

    <p><em>First‑order condition.</em>
    When densities exist at \(t\), \(R'(t)=C_{01}\pi\,f_1(t)-C_{10}(1-\pi)\,f_0(t)\).
    Setting \(R'(t^\star)=0\) yields
    \[
      \frac{f_1(t^\star)}{f_0(t^\star)}=\frac{C_{10}(1-\pi)}{C_{01}\pi}\;=:\;m^\star.
    \]
    But the ROC parameterization by threshold gives
    \(\dfrac{d\,\mathrm{TPR}(t)}{d\,\mathrm{FPR}(t)}
     =\dfrac{-f_1(t)}{-f_0(t)}=\dfrac{f_1(t)}{f_0(t)}\).
    Hence the Bayes point occurs where the ROC’s tangent slope equals \(m^\star\).</p>

    <p><em>Equal‑variance Gaussians.</em>
    If \(S\mid Y=i\sim\mathcal{N}(\mu_i,\sigma^2)\),
    \[
      \log\frac{f_1(t)}{f_0(t)}
      =\frac{\mu_1-\mu_0}{\sigma^2}\Big(t-\frac{\mu_0+\mu_1}{2}\Big).
    \]
    Solving \(\frac{f_1(t^\star)}{f_0(t^\star)}=m^\star\) gives
    \[
      t^\star=\frac{\mu_0+\mu_1}{2}+\frac{\sigma^2}{\mu_1-\mu_0}\,\ln m^\star
      =\frac{\mu_0+\mu_1}{2}+\frac{\sigma^2}{\mu_1-\mu_0}\,
        \ln\!\frac{(1-\pi)C_{10}}{\pi C_{01}}.
    \]
    Substituting \(t^\star\) back into \(f_1/f_0\) recovers \(m^\star\), verifying the slope condition at the optimum.</p>
  </li>

  <li><strong>Asymptotics/U‑statistics: variance of empirical AUC.</strong>
    <p><em>Estimator and unbiasedness.</em>
    With \(\{S_{1i}\}_{i=1}^m\overset{\text{i.i.d.}}{\sim}F_1\),
    \(\{S_{0j}\}_{j=1}^n\overset{\text{i.i.d.}}{\sim}F_0\), independent,
    \[
      \widehat{\mathrm{AUC}}
      =\frac{1}{mn}\sum_{i=1}^m\sum_{j=1}^n h(S_{1i},S_{0j}),
      \quad
      h(s_1,s_0)=\mathbf{1}\{s_1>s_0\}+\tfrac12\mathbf{1}\{s_1=s_0\}.
    \]
    Then \(\mathbb{E}[\widehat{\mathrm{AUC}}]
    =\mathbb{E}[h(S_1,S_0)]=\mathrm{AUC}\) by independence, so the estimator is unbiased.</p>

    <p><em>Hoeffding decomposition.</em>
    Define
    \[
      \psi_1(s):=\mathbb{E}[h(s,S_0)] 
      =\Pr(S_0<s)+\tfrac12\Pr(S_0=s)
      =F_0(s^-)+\tfrac12\Delta F_0(s),
    \]
    \[
      \psi_0(s):=\mathbb{E}[h(S_1,s)]
      =\Pr(S_1>s)+\tfrac12\Pr(S_1=s)
      =1-F_1(s^-)-\tfrac12\Delta F_1(s).
    \]
    Let \(\theta=\mathrm{AUC}\).
    The U‑statistic expansion is
    \[
      \widehat{\mathrm{AUC}}-\theta
      =\frac{1}{m}\sum_{i=1}^m\big(\psi_1(S_{1i})-\theta\big)
       +\frac{1}{n}\sum_{j=1}^n\big(\psi_0(S_{0j})-\theta\big)
       +\text{degenerate remainder},
    \]
    where the remainder is \(o_p(m^{-1/2}+n^{-1/2})\).</p>

    <p><em>Asymptotic variance and CLT.</em>
    With \(\sigma_1^2:=\operatorname{Var}(\psi_1(S_1))\),
    \(\sigma_0^2:=\operatorname{Var}(\psi_0(S_0))\),
    \[
      \operatorname{Var}(\widehat{\mathrm{AUC}})
      =\frac{\sigma_1^2}{m}+\frac{\sigma_0^2}{n}+o\!\Big(\frac{1}{m}+\frac{1}{n}\Big).
    \]
    If \(m,n\to\infty\) with \(m/(m+n)\to\lambda\in(0,1)\),
    \[
      \sqrt{\frac{mn}{m+n}}\big(\widehat{\mathrm{AUC}}-\theta\big)
      \;\xrightarrow{d}\; \mathcal{N}\!\big(0,\,(1-\lambda)\sigma_1^2+\lambda\sigma_0^2\big).
    \]</p>

    <p><em>Regularity.</em>
    i.i.d. within class and independence across classes; \(h\) bounded (here in \([0,1]\)), ensuring Lindeberg; \(\sigma_1^2+\sigma_0^2>0\) (i.e., non‑degenerate, excluding perfect separation).</p>
  </li>

  <li><strong>Distribution shift: AUC invariance; PR sensitivity.</strong>
    <p><em>Claim.</em> If the conditional score laws \(F_0,F_1\) are identical across domains, then the entire ROC and its area are invariant to changes in \(\pi\) or in the covariate distribution \(p(x)\) that leave \(F_i\) unchanged, while PR depends on \(\pi\).</p>

    <p><em>Proof (ROC/AUC).</em>
    For any threshold \(t\),
    \(\mathrm{TPR}(t)=1-F_1(t)\) and \(\mathrm{FPR}(t)=1-F_0(t)\) depend only on \(F_1,F_0\), not on \(\pi\).
    Therefore the parametric curve \(\{(\mathrm{FPR}(t),\mathrm{TPR}(t))\}_t\) and
    \(\mathrm{AUC}=\int (1-F_1)\,dF_0\) are invariant.</p>

    <p><em>PR depends on \(\pi\).</em>
    Precision at threshold \(t\) is
    \[
      \mathrm{Prec}(t)=\frac{\pi\,\mathrm{TPR}(t)}{\pi\,\mathrm{TPR}(t)+(1-\pi)\,\mathrm{FPR}(t)},
    \]
    so changing \(\pi\) alters \(\mathrm{Prec}(t)\) and hence AUPRC.</p>

    <p><em>Numeric illustration.</em>
    Fix \(\mathrm{TPR}=0.80\), \(\mathrm{FPR}=0.10\) at some \(t\).
    If \(\pi=0.50\): \(\mathrm{Prec}=0.4/(0.4+0.05)=0.888\).
    If \(\pi=0.10\): \(\mathrm{Prec}=0.08/(0.08+0.09)=0.471\).
    The ROC point is identical; precision shifts dramatically with prevalence.</p>
  </li>
</ol>

  </details>
</section>
